{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4F8zhvztqJJ"
      },
      "source": [
        "# Multi-Domain: Video Model\n",
        "\n",
        "This notebook shows the model and training process for the video-side of the multi-domain model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5zDTBCi27JQ",
        "outputId": "446e1682-abde-4acb-ffc9-b8cf507f6f5c"
      },
      "source": [
        "!apt-get install -y xxd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xxd is already the newest version (2:8.0.1453-1ubuntu1.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfTAOjguw2__"
      },
      "source": [
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnkpAIX-y7zN",
        "outputId": "ddb80807-3f5d-4dd4-9516-eb2e59ce46b8"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyPdzyGIt5nA"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data has been preprocessed into numpy arrays. This is done to save space on Google Drive. To do this yourself you can use `PIL` or `cv2` to import images and convert them to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQFpdP9Vy9Dx"
      },
      "source": [
        "!cp gdrive/MyDrive/multi_domain/video_dataset.npz ."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF8eRg2uzJMd"
      },
      "source": [
        "dataset = np.load(\"video_dataset.npz\")\n",
        "\n",
        "x_train = dataset['x_train']\n",
        "y_train = dataset['y_train']\n",
        "x_val = dataset['x_val']\n",
        "y_val = dataset['y_val']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PFqZcCwzMsc",
        "outputId": "1798954f-4dc8-483a-bcd6-3916bf78db8f"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(454, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydW4spH9uWVe"
      },
      "source": [
        "I had originally planned for a `(224, 224, 3)` image model. However because the Pico is so small this would require too much memory (even when represented with `int8` dtype). \n",
        "\n",
        "Instead the images are now converted use `PIL` to greyscale and then resized into `28x28` images. This results in arrays of the shape `(28, 28, 1)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2A9EdvovqpC"
      },
      "source": [
        "#create new temporary arrays\n",
        "x_train_new = []\n",
        "x_val_new = []\n",
        "\n",
        "for img in x_train:\n",
        "  #convert array into PIL Image\n",
        "  image = PIL.Image.fromarray(img)\n",
        "  #resize (this is current (28,28,3))\n",
        "  image = image.resize((28,28))\n",
        "  #convert to greyscale -> (28, 28, 1)\n",
        "  image = image.convert('L')\n",
        "  image = np.array(image)\n",
        "  #reshape for keras as this will give an array (28,28)\n",
        "  image = image.reshape((28,28, 1))\n",
        "  x_train_new.append(image)\n",
        "\n",
        "#repeat for validation dataset\n",
        "for img in x_val:\n",
        "  image = PIL.Image.fromarray(img)\n",
        "  image = image.resize((28,28))\n",
        "  image = image.convert('L')\n",
        "  image = np.array(image)\n",
        "  image = image.reshape((28,28, 1))\n",
        "  x_val_new.append(image)\n",
        "\n",
        "#set x train and x val to the new arrays\n",
        "x_train = np.array(x_train_new)\n",
        "x_val = np.array(x_val_new)\n",
        "\n",
        "del x_train_new\n",
        "del x_val_new\n",
        "\n",
        "#final standardise the images to be between 0-1\n",
        "x_train = x_train / 255\n",
        "x_val = x_val / 255"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YnsNhFyvrGx"
      },
      "source": [
        "We now need to convert the output data into integers to represent the classes. The two classes are \"happy\" and \"angry\". For a problem this simple, a list comprehension can simply be done to change \"happy\" into a 1 and \"angry\" into a 0.\n",
        "\n",
        "- Happy: 1\n",
        "- Angry: 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWNMXbsVzknN"
      },
      "source": [
        "y_train = np.array([1 if y == 'happy' else 0 for y in y_train])\n",
        "y_val = np.array([1 if y == 'happy' else 0 for y in y_val])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5eu7G_uwOfL"
      },
      "source": [
        "The last step for the data is to shuffle the dataset. This can help with overfitting as there are no clusters of the same input/output to overfit to when taking batches.\n",
        "\n",
        "Because this is a small dataset it can be done simply with the `random` module in the Python standard library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipi3nLL9mRUp"
      },
      "source": [
        "#create a list of tuples\n",
        "c = list(zip(x_train, y_train))\n",
        "#shuffle the tuples\n",
        "random.shuffle(c)\n",
        "#return back to x_train and y_train\n",
        "x_train, y_train = zip(*c)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjTkrPTfyVEo"
      },
      "source": [
        "x_train = np.array(x_train)\n",
        "x_val = np.array(x_val)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "y_train.shape = (len(y_train),1)\n",
        "y_val.shape = (len(y_val), 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt1fmkeRwtYO"
      },
      "source": [
        "## Modelling\n",
        "\n",
        "The next part is to create the neural network to fit to the images. Because this is a simple problem (binary classification) a model is built from scratch. However Tensorfow and Keras have many pretrained models which can be adapted to your problem. Prequantized models can also be found on the [TensorFlow Hub](https://tfhub.dev/s?q=quantized). \n",
        "\n",
        "When creating a model for a microcontroller you need to think more carefully about your model selection. A few important points:\n",
        "- Are the layers supported by TensorFlow Lite for Microcontrollers?\n",
        "- Is the model too big?\n",
        "- Is there a more efficient architecture\n",
        "\n",
        "For example when running on a laptop you may create a really simple dense network by flattening the image. This will result in too many weights for a microcontroller and waste precious memory. \n",
        "\n",
        "The model in this example is a simple feed-forward convolutional network which uses a sigmoid classifier to shift the output between 0 and 1 (like the y values we have)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuSNjyJdkV3N"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size = (2,2), activation=\"relu\", input_shape = (28,28,1)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(32, kernel_size= (2,2), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(64, kernel_size = (2,2), activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2y89RKkXOC1",
        "outputId": "75cf5217-49b8-4e79-810a-c587592e8796"
      },
      "source": [
        "model.build((None, 28,28, 1))\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 27, 27, 32)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 27, 27, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4128      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 5, 5, 64)          8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 5, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 13,313\n",
            "Trainable params: 13,057\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4f7UbOWyoSq"
      },
      "source": [
        "## Training\n",
        "\n",
        "The model is trained with a simple training schedule. The loss function is `binary_crossentropy` which is often used for binary classification problems. The optimizer is called `Adam`.\n",
        "\n",
        "We first use a large learning rate (this allows weights to jump around a lot at the start to avoid falling into local minimum. After 3 epochs the model then trains with a smaller learning rate to get closer to the minimum without \"jumping\" out for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a6WOTLlDHgv"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.1), metrics=['accuracy'])\n",
        "model.build((28,28,1))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCVwTmBQDKaX",
        "outputId": "aec2849d-0387-415e-cc8c-5d2ae2b60a44"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=3, batch_size=8, validation_data=(x_val, y_val), shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "57/57 [==============================] - 2s 19ms/step - loss: 0.9761 - accuracy: 0.8061 - val_loss: 63.8960 - val_accuracy: 0.5902\n",
            "Epoch 2/3\n",
            "57/57 [==============================] - 1s 11ms/step - loss: 0.3207 - accuracy: 0.9557 - val_loss: 3.5257 - val_accuracy: 0.5902\n",
            "Epoch 3/3\n",
            "57/57 [==============================] - 1s 11ms/step - loss: 0.0075 - accuracy: 0.9961 - val_loss: 17.7726 - val_accuracy: 0.4098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNbXn9fug-Ud",
        "outputId": "3f2850e8-c73c-434a-be37-14017484b11b"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0001), metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=8, validation_data=(x_val, y_val), shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0451 - accuracy: 0.9850 - val_loss: 19.0352 - val_accuracy: 0.4098\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 1s 11ms/step - loss: 0.0730 - accuracy: 0.9942 - val_loss: 16.5460 - val_accuracy: 0.4098\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.1078 - accuracy: 0.9830 - val_loss: 10.1901 - val_accuracy: 0.4098\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 1s 11ms/step - loss: 0.0614 - accuracy: 0.9920 - val_loss: 1.9561 - val_accuracy: 0.6557\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 3.7911e-06 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.1327 - accuracy: 0.9834 - val_loss: 1.3450e-08 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.0481 - accuracy: 0.9915 - val_loss: 2.2917e-14 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.5786e-18 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 5.6301e-21 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.0319 - accuracy: 0.9967 - val_loss: 1.1971e-20 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmE11XkZzJm-"
      },
      "source": [
        "## Post-Training Quantization\n",
        "\n",
        "The model is quantized ready for use. This helps keep the model as small as possible. Here we use int8 quantization to keep it as small as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRidSQIMzhXA"
      },
      "source": [
        "def representative_dataset():\n",
        "  for data in x_train:\n",
        "    data = data.reshape(1, 28, 28, 1)\n",
        "    yield [data.astype(np.float32)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB0cYDYfDPUA",
        "outputId": "3be84626-5c90-4baf-f1b3-11df221631f7"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8 \n",
        "converter.inference_output_type = tf.int8\n",
        "quant_model = converter.convert()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpm98l4ro3/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9FJm1Ozmt8"
      },
      "source": [
        "Once converted, the model can be exported as a .tflite. For tensorflow lite for microcontrollers. One final step is required and that is to convert the model into a .cc file using `xxd`.  The C source file contains the TensorFlow Lite model as a char array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZRPkkIJDj3E"
      },
      "source": [
        "with open(\"video_model_mobilenet.tflite\", \"wb\") as f:\n",
        "  f.write(quant_model)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BGvpQZOHRoQ"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"video_model_mobilenet.tflite\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuPl9Z_WHZPG",
        "outputId": "b966ce0e-7450-464e-faf4-973674e39796"
      },
      "source": [
        "print(p.stat().st_size / 1024, \"kb\", (p.stat().st_size) / (1024*1024), \"mb\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.7578125 kb 0.02320098876953125 mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKwVauvV1L5-"
      },
      "source": [
        "!xxd -i \"video_model_mobilenet.tflite\" > \"video_model_mobilenet.cc\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCbj_-Hr0rA_"
      },
      "source": [
        "## Test Data\n",
        "\n",
        "To test the model on the microcontroller, we can create some test data which can be used to simulate the model.\n",
        "\n",
        "To do this we need to convert the float32 input data into int8 data using the conversion provided with the Tensorflow Lite for Microcontrollers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8_QttJ1Cxt"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(\"video_model_mobilenet.tflite\")\n",
        "input_details = interpreter.get_input_details()\n",
        "\n",
        "scale, zero = input_details[0][\"quantization\"]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ait0m_n_0x0z"
      },
      "source": [
        "test_x_data = (x_val[0] / scale + zero).astype(input_details[0]['dtype'])\n",
        "test_x_data.tofile(\"x_video_test.txt\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juvlw6rf2CRs"
      },
      "source": [
        "!xxd -i x_video_test.txt > x_video_test.cc"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}