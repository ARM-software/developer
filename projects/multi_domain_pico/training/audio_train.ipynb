{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-5VqyXB4Kbp"
      },
      "source": [
        "# Multi-Domain: Audio Model\n",
        "\n",
        "The notebook shows the models and training process for the audio-side of the multi-domain model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7wc8oy-4WZA",
        "outputId": "c77d20c1-28d5-4950-de3d-74f80be04ea4"
      },
      "source": [
        "!apt-get install -y xxd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xxd is already the newest version (2:8.0.1453-1ubuntu1.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28aVRRlkoXQO"
      },
      "source": [
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GygNTbpBPs",
        "outputId": "a588e5f1-86ed-4e3a-9cdd-cb634284d71c"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AWi4C654rMm"
      },
      "source": [
        "## Data\n",
        "\n",
        "The audio data is taken from the same video as the image data. The following preprocessing steps were taken to ensure consistency between all data samples.\n",
        "\n",
        "- Setting the same sample rate\n",
        "- Setting the same audio length\n",
        "- Conversion into mfccs with `python-speech-features` package\n",
        "\n",
        "After the preprocessing the data was saved into numpy arrays ready for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyOokNbMpJ3F"
      },
      "source": [
        "!cp gdrive/MyDrive/multi_domain/audio_dataset.npz ."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOiSkPeYpSQJ"
      },
      "source": [
        "dataset = np.load(\"audio_dataset.npz\")\n",
        "\n",
        "x_train = dataset['x_train']\n",
        "y_train = dataset['y_train']\n",
        "x_val = dataset['x_val']\n",
        "y_val = dataset['y_val']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYWuN70S5Yvf"
      },
      "source": [
        "Set the output values to be 1 if \"happy\" and 0 if \"angry\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZwyk7itY7r"
      },
      "source": [
        "y_train = np.array([1 if y == 'happy' else 0 for y in y_train])\n",
        "y_val = np.array([1 if y == 'happy' else 0 for y in y_val])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8eYj9C5gP9"
      },
      "source": [
        "To be used by a CNN, the input arrays need to be reshaped to be similar to an image. One way to look at this is to imaging the mfccs as a greyscale image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIZaVemyplTy",
        "outputId": "e02db0d7-50cb-4278-f314-74f56c2fb5fd"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48, 16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgf-UDnLpwmm"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], \n",
        "                          x_train.shape[1], \n",
        "                          x_train.shape[2], \n",
        "                          1)\n",
        "x_val = x_val.reshape(x_val.shape[0], \n",
        "                      x_val.shape[1], \n",
        "                      x_val.shape[2], \n",
        "                      1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ijDwwGkpzed",
        "outputId": "dcfe1f5f-32db-4858-acb7-d0916d315ac2"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48, 16, 16, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBTapOV5rS_"
      },
      "source": [
        "#create a list of tuples\n",
        "c = list(zip(x_train, y_train))\n",
        "#shuffle the tuples\n",
        "random.shuffle(c)\n",
        "#return back to x_train and y_train\n",
        "x_train, y_train = zip(*c)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVcfE8J891wE"
      },
      "source": [
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnv2j2pZ5xvi"
      },
      "source": [
        "## Modelling\n",
        "\n",
        "The next part is to create the neural network to fit to the images. Because this is a simple problem (binary classification) a model is built from scratch. However Tensorfow and Keras have many pretrained models which can be adapted to your problem. Prequantized models can also be found on the [TensorFlow Hub](https://tfhub.dev/s?q=quantized). \n",
        "\n",
        "When creating a model for a microcontroller you need to think more carefully about your model selection. A few important points:\n",
        "- Are the layers supported by TensorFlow Lite for Microcontrollers?\n",
        "- Is the model too big?\n",
        "- Is there a more efficient architecture\n",
        "\n",
        "For example when running on a laptop you may create a really simple dense network by flattening the image. This will result in too many weights for a microcontroller and waste precious memory. \n",
        "\n",
        "The model in this example is a simple feed-forward convolutional network which uses a sigmoid classifier to shift the output between 0 and 1 (like the y values we have)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ7HE59Hp1Vy"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size = (2,2), activation=\"relu\", input_shape = (16,16,1)),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(32, kernel_size = (2,2), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(64, kernel_size = (2,2), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9hnbcFVrdc1",
        "outputId": "703ccfe7-f6ed-4942-981d-78ca17263e81"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='rmsprop', \n",
        "              metrics=['acc'])\n",
        "model.build((1,16,16,1))\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 15, 15, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 6, 6, 32)          4128      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 64)          8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 16,769\n",
            "Trainable params: 16,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO8ovqMc6DNu"
      },
      "source": [
        "## Training\n",
        "\n",
        "Because the model is so simple, the model is simple trainined for 30 epochs at the default learning rate for Keras's `'rmsprop'` optimization function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uhXnsf_q65p",
        "outputId": "b41db091-c91f-47c7-a3e3-bd55aec9b662"
      },
      "source": [
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    epochs=30, \n",
        "                    batch_size=8, \n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 1.0502 - acc: 0.4432 - val_loss: 0.6779 - val_acc: 0.5455\n",
            "Epoch 2/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.7625 - acc: 0.3949 - val_loss: 0.6875 - val_acc: 0.4545\n",
            "Epoch 3/30\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.7001 - acc: 0.4461 - val_loss: 0.6699 - val_acc: 0.5455\n",
            "Epoch 4/30\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6763 - acc: 0.5452 - val_loss: 0.6714 - val_acc: 0.5455\n",
            "Epoch 5/30\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.6986 - acc: 0.5256 - val_loss: 0.7308 - val_acc: 0.4545\n",
            "Epoch 6/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.7516 - acc: 0.4324 - val_loss: 0.6534 - val_acc: 0.5455\n",
            "Epoch 7/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5472 - acc: 0.7798 - val_loss: 0.6427 - val_acc: 1.0000\n",
            "Epoch 8/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.5694 - acc: 0.6857 - val_loss: 0.6069 - val_acc: 0.5455\n",
            "Epoch 9/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.7299 - acc: 0.4759 - val_loss: 0.5991 - val_acc: 1.0000\n",
            "Epoch 10/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5893 - acc: 0.7354 - val_loss: 0.6099 - val_acc: 1.0000\n",
            "Epoch 11/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5262 - acc: 0.7345 - val_loss: 0.6070 - val_acc: 1.0000\n",
            "Epoch 12/30\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.5098 - acc: 0.7842 - val_loss: 0.7074 - val_acc: 0.4545\n",
            "Epoch 13/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4821 - acc: 0.7226 - val_loss: 0.7202 - val_acc: 0.4545\n",
            "Epoch 14/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.6017 - acc: 0.7220 - val_loss: 0.5694 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4888 - acc: 0.7783 - val_loss: 0.7010 - val_acc: 0.4545\n",
            "Epoch 16/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3970 - acc: 0.8491 - val_loss: 1.2067 - val_acc: 0.4545\n",
            "Epoch 17/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4748 - acc: 0.8512 - val_loss: 0.8220 - val_acc: 0.4545\n",
            "Epoch 18/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4867 - acc: 0.7619 - val_loss: 0.5859 - val_acc: 0.4545\n",
            "Epoch 19/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3115 - acc: 0.9536 - val_loss: 0.7051 - val_acc: 0.4545\n",
            "Epoch 20/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3172 - acc: 0.8869 - val_loss: 0.7480 - val_acc: 0.4545\n",
            "Epoch 21/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3835 - acc: 0.7708 - val_loss: 0.4823 - val_acc: 0.4545\n",
            "Epoch 22/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2908 - acc: 0.8417 - val_loss: 1.0031 - val_acc: 0.4545\n",
            "Epoch 23/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3345 - acc: 0.8497 - val_loss: 0.5579 - val_acc: 0.4545\n",
            "Epoch 24/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3693 - acc: 0.8408 - val_loss: 0.6375 - val_acc: 0.4545\n",
            "Epoch 25/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3699 - acc: 0.8289 - val_loss: 0.4651 - val_acc: 0.4545\n",
            "Epoch 26/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1903 - acc: 0.9750 - val_loss: 1.4136 - val_acc: 0.4545\n",
            "Epoch 27/30\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3907 - acc: 0.8262 - val_loss: 0.9264 - val_acc: 0.4545\n",
            "Epoch 28/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2175 - acc: 0.9104 - val_loss: 0.8158 - val_acc: 0.4545\n",
            "Epoch 29/30\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2668 - acc: 0.8429 - val_loss: 1.0772 - val_acc: 0.4545\n",
            "Epoch 30/30\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1814 - acc: 0.9024 - val_loss: 0.3030 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neCAPkq96AWT"
      },
      "source": [
        "## Post-Training Quantization\n",
        "\n",
        "The model is quantized ready for use. This helps keep the model as small as possible. Here we use int8 quantization to keep it as small as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W552X-WEwJ_5"
      },
      "source": [
        "def representative_dataset():\n",
        "  for data in x_train:\n",
        "    yield [x_train.astype(np.float32)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOhkbdJRvtrn",
        "outputId": "8470215f-35a4-4611-887f-dc6771fc51eb"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "quant_model = converter.convert()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp4itqsb52/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSH1jscewbiE"
      },
      "source": [
        "with open(\"audio_model.tflite\", \"wb\") as f:\n",
        "  f.write(quant_model)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq5lKdhO7K5s"
      },
      "source": [
        "!xxd -i \"audio_model.tflite\" > \"audio_model.cc\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1TLd5N69ZC"
      },
      "source": [
        "Once converted, the model can be exported as a .tflite. For tensorflow lite for microcontrollers. One final step is required and that is to convert the model into a .cc file using `xxd`.  The C source file contains the TensorFlow Lite model as a char array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTZP67f-7R_e"
      },
      "source": [
        "## Test Data\n",
        "\n",
        "To test the model on the microcontroller, we can create some test data which can be used to simulate the model.\n",
        "\n",
        "To do this we need to convert the float32 input data into int8 data using the conversion provided with the Tensorflow Lite for Microcontrollers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESi25PxFLTDj"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(\"audio_model.tflite\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ktq9nH_7PKR"
      },
      "source": [
        "input_details = interpreter.get_input_details()\n",
        "scale, zero = input_details[0][\"quantization\"]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iQgEcGX7ZBi"
      },
      "source": [
        "test_x_data = (x_val[0] / scale + zero).astype(input_details[0]['dtype'])\n",
        "test_x_data.tofile(\"x_audio_test.txt\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcq0hHiYLV55"
      },
      "source": [
        "!xxd -i x_audio_test.txt > x_audio_test.cc"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}